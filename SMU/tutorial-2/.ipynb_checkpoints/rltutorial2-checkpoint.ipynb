{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Gym library and the game we will play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from game import *\n",
    "import gym\n",
    "from gym.envs.registration import register\n",
    "from gym import wrappers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the environment and create an instance of it. The environment is a roulette game. You enter the casino with a random number of tokens between $0$ and $10$. Then you bet as in a normal roulette. The roulette wheel has $37$ spots. If the bet is $0$ and $0$ comes up, you win a reward of $35$ tokens. If the parity of your bet matches the parity of the spin, you win $1$ token. Otherwise, you lose $1$ token.\n",
    "\n",
    "The observation provided by the game consists of two parts:\n",
    "<ol>\n",
    " <li> Your cumulative reward so far.\n",
    " <li> The last number that has fallen on the roulette.\n",
    "</ol>\n",
    "This representation is not perfect for reinforcement learning settings; however, it illustrates some common problems with reinforcement learning algorithms.\n",
    "\n",
    "Action $37$ means that you want to cash your tokens and walk away. Your observation will still contain the number of tokens you held and the next outcome on the roulette. The casino does not allow you to play on loan, therefore if you own $0$ tokens, the game automatically ends.\n",
    "\n",
    "The problem is the following - cassino is cheating and the roulette is false. When you own ten or more tokens, the dealer secretely decreases your winning probability.  Also if you own more than $20$ tokens, the security becomes suspicious and expels you from the casino. Use TD-learning to show that the roulette is not fair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "register(\n",
    "    id='smu-rl-roulette2019-v0',\n",
    "    entry_point='game:RouletteEnv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "envsimple = gym.make('smu-rl-roulette2019-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> The passive reinforcement learning agent using temporal difference. </h1>\n",
    "\n",
    "First, we will implement an agent with a fixed learning rate. We will use a fixed strategy and observe how well it performs. The policy with $20\\,\\%$ walks away and changes the tokens into reward. Otherwise, the agent randomly picks a number; all have the same probability. The policy is not deterministic in this case, however, TD-learning will work anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def policy(observation):\n",
    "    if observation[1] > 27 or np.random.random() < 0.2:\n",
    "        return 37\n",
    "    return np.random.randint(0, 37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we will use a fixed learning rate to see how the policy performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement the agent. The TD method follows the pseudocode below:\n",
    "<ol> <li> Repeat (for each episode):\n",
    "     <ol> <li> Initialize $s$ as the start state.\n",
    "          <li> Repeat (for each step):\n",
    "          <ol> <li> $a \\gets$ action given by $\\pi$ for $s$\n",
    "               <li> Take action $a$; observe reward, $r$, and the next state $s'$\n",
    "               <li> $U(s) \\gets U(s) + \\alpha \\left( r + \\gamma U(s') - U(s) \\right)$\n",
    "               <li> $s \\gets s'$\n",
    "          </ol>\n",
    "          <li> until $s$ is terminal\n",
    "      </ol>\n",
    "</ol>\n",
    "The pseudocode is taken from <a href=\"https://mitpress.mit.edu/books/reinforcement-learning\">Sutton, Barto book, figure 6.1</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to initialize the number of episodes and discount factor. Pick your own values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "number_of_epochs = 100 # TODO pick your own number and discount factor\n",
    "discount_factor = 0.5# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The template for the code is provided and is similar to the one you have in your project. Modify the code as handy. Documentation is available on <a href=\"https://gym.openai.com/docs/\">https://gym.openai.com/docs/</a>. You already see method <code>env.reset</code> and <code>env.render</code>. The last important method we will need is <code>env.step</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-29bf037f16d9>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-29bf037f16d9>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    U = # TODO : define utility function, i.e., a dictionary, an array or anything else\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "env = wrappers.Monitor(envsimple, 'smurltutorial', force=True, video_callable=False)\n",
    "U = # TODO : define utility function, i.e., a dictionary, an array or anything else\n",
    "\n",
    "for i in range(number_of_epochs):\n",
    "    observation = env.reset()\n",
    "    # TODO your code here\n",
    "    # in each step you will need to call method env.step() with the action given by the policy() method above\n",
    "    \n",
    "    #env.render() will show you the state of the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Convergence of utility values</h1>\n",
    "\n",
    "Now when the code works copy it to the cell below and checks whether the utility values converged. In each iteration store the utility of state $(10,1)$. Store the maximum update made to $U(s)$ for any $s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "env = wrappers.Monitor(envsimple, 'smurltutorial', force=True, video_callable=False)\n",
    "U = # TODO : define utility function\n",
    "Us101 = np.zeros(number_of_epochs)\n",
    "maxDelta = np.zeros(number_of_epochs)\n",
    "\n",
    "for i in range(number_of_epochs):\n",
    "    observation = env.reset()\n",
    "    # TODO your code here\n",
    "    #env.render()\n",
    "    maxDelta[i] = # TODO : store the maximum update to U(e)\n",
    "    Us101[i] = # TODO :  store the value of a state (you can pick a different one if you want)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will render the utility over time. First, we need to import matplotlib. Use <code>pip install matplotlib</code> (unless you did on the last tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this method to plot a vector of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_series(arr, fileName = None):\n",
    "    plt.plot(arr)\n",
    "    if fileName is not None:\n",
    "        plt.savefig(fileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the $U$ value of the state $(10,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plot_series(Us101)\n",
    "# put a name of a file as a second parameter if you want to save the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would expect the value to be around $10$. The result may depend on the value of the learning rate. If you get a plot like the following one, the values did not converge.\n",
    "<center>\n",
    "  <img src=\"not_converged.png\">\n",
    "</center>\n",
    "However, if your results look like the one below, the value for this state is correct. The values actually don't converge since we set a constant learning rate. The average, however, converges.\n",
    "<center>\n",
    "    <img src=\"converged.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of maximum update to the state utility function is explanatory as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plot_series(maxDelta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot should look like the one below.\n",
    "<center>\n",
    "  <img src=\"delta_constant_alpha.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utility function did not converge. However <b>the average values </b> of $U(s)$ converge. (optionally check yourself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: optionally check that the average values of $U(s)$ converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we may check the value of the state when we own ten (or nine tokens):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "U101 = # read from U value of state when you own 10 tokens and there is one on the roulette\n",
    "U091 = # the same for 9 tonens and one on the roulette\n",
    "\n",
    "print(U101)\n",
    "print(U091)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strange, isn't it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Decrease the learning rate </h1>\n",
    "If your state utility function converged in the last section, you are lucky. However, we know the solution from the lecture - the value of learning rate $\\alpha$ should be decreasing with the number of trials. More specifically, with the number of visits to the current state. In the last plot, the maximum change was approximately constant over time. As a result, the utility values oscillate.\n",
    "\n",
    "Therefore, copy the code from the last section to the cell below and choose some function so that the value of learning rate $\\alpha$ decreases with the number of visits of state $s$. [Answer the following question yourself: Why should be learning rate different for each state $s$?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "env = wrappers.Monitor(envsimple, 'smurltutorial', force=True, video_callable=False)\n",
    "U = # TODO : define utility function\n",
    "Ns = # TODO : this time you have to store the number of visits of each state\n",
    "Us101 = np.zeros(number_of_epochs)\n",
    "maxDelta = np.zeros(number_of_epochs)\n",
    "\n",
    "for i in range(number_of_epochs):\n",
    "    observation = env.reset()\n",
    "    # TODO your code here\n",
    "    #env.render()\n",
    "    maxDelta[i] = # TODO : store the maximum update to U(s)\n",
    "    Us101[i] = # TODO :  store the value of a state (you can pick a different one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your code again, this time with $\\alpha$ decreasing with the number of visits of a state. Check the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plot_series(Us101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your state utility function looks like the one below, you won, because the values converged. Also, the value looks reasonable. If not, try again.\n",
    "<center>\n",
    "  <img src=\"converged_decreasing_alpha.png\">\n",
    "</center>\n",
    "Hint (select the text to read - it is in white): <span style=\"color:white\">In AIMA they use $$\\alpha = \\frac{c}{c - 1 + \\mbox{number of visits}}$$ for a constant $c$\n",
    "\n",
    "Generally, the $\\alpha$ parameter should be selected so that $\\sum_t \\alpha_t$ diverges and $\\sum_t \\alpha_t^2$ converges.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Plot the maximum change in the state utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plot_series(maxDelta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the update is decreasing (as expected).\n",
    "<center>\n",
    "  <img src=\"delta_decreasing_alpha.png\">\n",
    "</center>\n",
    "However, in this case, the result is not nice as the maximum is not a robust statistics. Why do we see such high peaks even after $1 000 000$ games played? Compare with the following picture.\n",
    "<center>\n",
    "    <img src=\"delta_decreasing_alpha_last_year.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Further work </h1>\n",
    "\n",
    "If you got to this point, you are able to continue yourself without detailed instructions. Here are some ideas what you may want to do in the remaining time:\n",
    "<ul>\n",
    "  <li> Go to <code>game.py</code> file and change the number of states. Does the TD method scale? What is the maximum number of states?\n",
    "  <li> On <a href=\"https://gym.openai.com/\">https://gym.openai.com/</a> you may find plenty of environments. Pick one and pick one of the strategies that were submitted to the page. Then estimate the state utility function of the strategy.\n",
    "  <li> Implement the adaptive dynamic programming algorithm and compare it to the TD.\n",
    "  <li> Whatever you are interested in ...\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
